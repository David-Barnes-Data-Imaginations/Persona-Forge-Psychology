# config.yaml
model:
  name: "/models/gpt-oss-20b-4bit"  # Use local path instead of HF model ID
  cache_dir: "/models"
  use_local: true  # Flag to indicate local model usage

generation:
  max_new_tokens: 2048
  max_input_length: 6144      # Context window - THIS IS THE SETTING
  temperature: 0.7
  top_p: 0.9

server:
  host: "0.0.0.0"
  port: 8000
  workers: 1

security:
  api_key_required: false  # Set to true for production
  allowed_origins: ["*"]   # Restrict for production

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
