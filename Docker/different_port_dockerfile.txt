
services:
  data-agent:
    build:
      context: ..
      dockerfile: Docker/Dockerfile
    image: persona-forge:latest
    container_name: persona-forge
    network_mode: "host"  # Use host network to access host's Ollama instance
    ports:
      - "7860:7860"   # Gradio interface
      - "8000:8000"   # FastAPI if needed
      # Port for Ollama server removed as we're using the host's Ollama

    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - HF_TOKEN=${HF_TOKEN}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - PYTHONUNBUFFERED=1
      - USE_HOST_OLLAMA=true  # Flag to indicate we should use host's Ollama

    command: ["python", "main.py"]

# Ollama service removed since we're using the host's Ollama instance

# No volumes needed since we're not running Ollama in Docker