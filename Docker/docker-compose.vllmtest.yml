version: "3.9"

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "8005:8000"   # gRPC (container 8001) is optional; see command below
      - "8006:8001"   # HTTP → host 8006
      - "8007:8002"   # Metrics → host 8007
    volumes:
      # Point this at your HF model folder with config.json, tokenizer, and sharded model.safetensors
      - /ABS/PATH/TO/HF_MODEL_DIR:/models/gpt-oss-20b:ro
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model /models/gpt-oss-20b
      --dtype float16
      --host 0.0.0.0
      --port 8001
      --grpc-port 8000
      --metrics-port 8002
      --gpu-memory-utilization 0.90
      --max-model-len 8192
      --swap-space 32
      --enforce-eager
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8001/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
